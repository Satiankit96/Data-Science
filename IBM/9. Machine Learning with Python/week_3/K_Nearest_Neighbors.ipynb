{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53861f6-10ee-4643-a36b-7331fbcf17e0",
   "metadata": {},
   "source": [
    "### Introduction to classification \n",
    "- Supervised machine learning approach.  Unkown items into discreet categories/ Classes.\n",
    "- The target variable is a categorical variable with discreet value.\n",
    "- Loan prediction(0/1)\n",
    "- this is a binary classification classifier \n",
    "\n",
    "![](class.png)\n",
    "\n",
    "- Multi class classification (Which drug works well)\n",
    "\n",
    "![](drug.png)\n",
    "\n",
    "![](types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1395ad30-f2f9-4e9c-a845-53324041bb58",
   "metadata": {},
   "source": [
    "## K Nearest neighbours \n",
    "- takes a bunch of labeled points and uses them to learn how to label other points. This algorithm classifies cases based on their similarity to other cases.\n",
    "\n",
    "  K=  5 (3: plus customers )\n",
    "![](plot.png)\n",
    "\n",
    "\n",
    "![](algo.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3dc65e-7d85-4dfa-90f4-0d895d0f8cd9",
   "metadata": {},
   "source": [
    "### Choosing a right value of K\n",
    "- Low value means a overfitted model\n",
    "- high value of K makes the model generalised\n",
    "- K =1 and use iteration to find out the most optimal value of K. (Just like calculating for the highest value of R^2 in Regression)\n",
    "\n",
    "## evaluation metrics for classification  \n",
    "\n",
    "1. F1 Score - Creating aconfusion module (Matix of true positive and fals negatives)\n",
    "\n",
    "![](F1.png)\n",
    "\n",
    "2. jacqard score - Venn diagram\n",
    "3. Log loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a831d2e6-fa56-40f7-ae88-293c2d6f5d51",
   "metadata": {},
   "source": [
    "## decision Trees\n",
    "\n",
    "- Can be considered a multi classification model\n",
    "- Different classes at each level\n",
    "\n",
    "- Trees are buil by finding significant features\n",
    "- Sig features - What features are the ones that divide the dataset int pure parts\n",
    "- Entropy of data - Randomness in the data. So the lower the better (0-1)0 = All of a kind, 1 : A =B\n",
    "- Information Gain = (Entropy before split - Weighed entropy after split) - This should be maximised "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6913ac85-e2da-4427-b8a2-54e0cd8547e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
